# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178LKMxzoAtKDe27QP6dbSoWJB3ASPXTQ
"""

import pandas as pd
from random import seed
from math import exp
from itertools import repeat
from math import floor
from sklearn.metrics import confusion_matrix,classification_report
import numpy as np
from sklearn.model_selection import train_test_split
from random import randrange

class SVM_PREDICTION():

  def getting_Data_Point(self,temp_x):
    statistical_data = []

    features_list = list(zip(*temp_x))

    for features in features_list:
      temp_list = []
      temp_list.append(min(features))
      temp_list.append(max(features))
      statistical_data.append(temp_list)


    for each_data_point in temp_x:
      for iter in range(len(each_data_point)):
        numerator = (each_data_point[iter] - statistical_data[iter][0])
        denominator = (statistical_data[iter][1] - statistical_data[iter][0])
        each_data_point[iter] = numerator / denominator

  def calculatin_Hyperparamenter(self,earlier_hyperparamter,x_train,y_classes,learning_rate,hyperParameter,regularization_paramter):
      length_of_early = len(earlier_hyperparamter)
      test_data =0
      for each_iter in range(length_of_early):
        test_data =0
        if length_of_early > 0:
          test_data = 100
        else:
          test_data = 200

        accumulate_value = 0 
        for train_iter in range(len(x_train)):
          delta = 0.0
          x_intial = x_train[train_iter]
          for del_cal in range(len(earlier_hyperparamter)):
            delta = delta + earlier_hyperparamter[del_cal]*x_intial[del_cal]

          accumulate_value = accumulate_value + (self.sigmoid(delta) - y_classes[train_iter])*x_train[train_iter][each_iter]
        
        training_values = 0
        if test_data == 100:
          training_values = 0
        else:
          training_values = 2000

        hyperParameter[regularization_paramter][each_iter] = hyperParameter[regularization_paramter][each_iter] - learning_rate*accumulate_value


  def sigmoid(self,z_index_value):
    return 1.0/(1.0 + exp(-z_index_value))
  
  def calulate_prediction(self,y_pred, y_test):
    y_pred_list = []
    y_test_list = []
    for y_temp_pred, y_temp_test in zip(y_pred,y_test):
      if y_temp_pred[0]==1:
        y_pred_list.append(0)
      else:
        y_pred_list.append(1)
      
      if y_temp_test[0]==1:
        y_test_list.append(0)
      else:
        y_test_list.append(1)

    print(" Confusion Matrix " ,confusion_matrix(y_test_list,y_pred_list))
    tn, fp, fn, tp = confusion_matrix(y_test_list, y_pred_list).ravel()
    preci = tp/(tp+fp)
    recal = tp/(tp+fn)
    
    print(classification_report(y_test_list,y_pred_list))
    return preci,recal

  def fit(self,x,y,list_of_labels,test_train_split,validations,learning_rate,iteration):

    list_of_accuracies = []
    
    for each_iter in range(validations):
      number_of_classes = []
      seed(each_iter+1)
      x_test = x.tolist()
      y_test = y.tolist()
    
      
      negate_split = 1 - test_train_split
      
      
      x_train = []
      length_test = len(x_test)
      train_size = floor(negate_split*length_test)
      y_train = []
    
      while(len(x_train) < train_size):
        index = randrange(len(x_test))
        x_test_value = x_test.pop(index)
        temp_values_test  =0
        y_test_value = y_test.pop(index)
        
        if train_size > 0:
          temp_values_test = 100
        else:
          temp_values_test = 200
          
        x_train.append(x_test_value)
        y_train.append(y_test_value)
      
      for iter in range(len(list_of_labels)):
        temp = []
        for iter_check in y_train:
          temp.append(iter_check[iter])
        
        number_of_classes.append(temp)

      hyperParameter=[]
      for _ in range(len(number_of_classes)):
        value_temp =list(repeat(0, len(x_train[0])))
        hyperParameter.append(value_temp)


      for i in range(iteration):
        for each_class_iter in range(len(number_of_classes)):
          earlier_hyperparamter = hyperParameter[each_class_iter]
          self.calculatin_Hyperparamenter(earlier_hyperparamter,x_train,number_of_classes[each_class_iter],learning_rate,hyperParameter,each_class_iter)

      y_pred = self.predict(x_test,hyperParameter)
      y_pred_length = len(y_pred)
      precision ,recall = self.calulate_prediction(y_pred, y_test)
      right_guess = 0 
      for iter in range(y_pred_length):
        prediction_values = y_pred[iter]
        actual_values = y_test[iter]
        if(prediction_values==actual_values):
          right_guess = right_guess + 1

      temp_accuracy = right_guess/y_pred_length


      list_of_accuracies.append(temp_accuracy)
      
      print("Precision: ", precision)
      print("recall: ",recall)
      #tn, fp, fn, tp = confusion_matrix(y_test, y_pred)
      print("Validation",each_iter+1,"accuracy score: ",list_of_accuracies[each_iter])
      #confusionMatrix = [tn,fp,fn,tp]
    return sum(list_of_accuracies)/len(list_of_accuracies)

  def predict(self,data,hyperParameter):
    values_of_count = 1
    list_of_prediction = []
    for first_iter in data:
      temporary_parameter = []
      checking_class = [0]*len(hyperParameter)
      for range_iter in range(len(hyperParameter)):
        delta_check = 0.0
        x_intial = hyperParameter[range_iter]
        length_of_an_iter =len(first_iter)
        for del_cal in range(length_of_an_iter):
          first_iter_value = first_iter[del_cal]

          value_of_x = x_intial[del_cal]

          total_value = first_iter_value * value_of_x
          
          delta_check = delta_check + total_value

        temp_sigmod_value=self.sigmoid(delta_check)
        temporary_parameter.append(temp_sigmod_value)
      
      max_value = max(temporary_parameter)
      training_dataset = 0
      if max_value > 0:
        training_dataset = 2000
      else:
        training_dataset = 4000
      position_in_list = temporary_parameter.index(max_value)
      checking_class[position_in_list] = 1
      list_of_prediction.append(checking_class)
      values_of_count = values_of_count + 1
    
    return list_of_prediction

  def preprocessing(self):
    print("Running Movie Success using SVM\n")
    url = "movies.csv"
    dataset = pd.read_csv(url)
    dataset.dropna(axis=0, inplace=True)
    x = pd.DataFrame(dataset,columns=['budget','genres','original_language','popularity','revenue','vote_average','vote_count'])
    
    x['genres'] = pd.Categorical(x['genres']).codes
    x['original_language'] = pd.Categorical(x['original_language']).codes


    y = dataset.iloc[:,20:21]
    y['Verdict'] = pd.Categorical(y['Verdict']).codes
    x_data  = x.values
    y = y.values

    y=y.reshape(-1)
    self.getting_Data_Point(x_data)
    return x_data,y

  def creatingLabels(self,y):
  
    newList = []
    list_of_labels = list(set(y))
    list_of_labels.sort()
    for num_iter in range(len(y)): 
      value = [0]
      length_of_labels = len(list_of_labels)
      created_one = value*length_of_labels
      length_of_labels = len(list_of_labels)
      test_dataset = 0
      if length_of_labels > 0:
        test_dataset = 100
      else:
        test_dataset = 2000

      index_of_created_one = list_of_labels.index(y[num_iter])
      created_one[index_of_created_one] = 1
      newList.append(created_one)
      y_data = np.asarray(newList)
      return y_data,list_of_labels

svm_pred = SVM_PREDICTION()
print("Running Movie Success using SVM\n")
url = "movies.csv"
dataset = pd.read_csv(url)
dataset.dropna(axis=0, inplace=True)
x = pd.DataFrame(dataset,columns=['budget','genres','original_language','popularity','revenue','vote_average','vote_count'])
x['genres'] = pd.Categorical(x['genres']).codes
x['original_language'] = pd.Categorical(x['original_language']).codes


y = dataset.iloc[:,20:21]
y['Verdict'] = pd.Categorical(y['Verdict']).codes
x_data  = x.values
y = y.values

y=y.reshape(-1)


svm_pred.getting_Data_Point(x_data)

list_of_labels = list(set(y))
list_of_labels.sort()
newList = []

for num_iter in range(len(y)): 
  value = [0]
  length_of_labels = len(list_of_labels)
  created_one = value*length_of_labels
  length_of_labels = len(list_of_labels)
  test_dataset = 0
  if length_of_labels > 0:
    test_dataset = 100
  else:
    test_dataset = 200

  index_of_created_one = list_of_labels.index(y[num_iter])
  created_one[index_of_created_one] = 1
  newList.append(created_one)

y_data = np.asarray(newList)
end_accuracy = svm_pred.fit(x_data,y_data,list_of_labels,test_train_split=0.2,validations=1,learning_rate=0.01,iteration=300)

print("\nFinal Statistics")
print("Model used: ","Linear SVM using Gradient Descent")
print("Accuracy: ",end_accuracy*100,"%")
print("No. of features: ", len(x_data[0]))