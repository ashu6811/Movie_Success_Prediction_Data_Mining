# -*- coding: utf-8 -*-
"""SVM_Movie_Success_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/178LKMxzoAtKDe27QP6dbSoWJB3ASPXTQ
"""

# INSE 6180 Security and Privacy Implications of Data Mining
# Group #17 Hit or Flop: Movie Success Prediction Using Data Mining Techniques
# SVM IMPLEMENTATION 2019 FALL

import pandas as pd
from random import seed
from math import exp
from itertools import repeat
from math import floor
from sklearn.metrics import confusion_matrix,classification_report
import numpy as np
from sklearn.model_selection import train_test_split
from random import randrange
from sklearn.metrics import roc_curve, auc

# Class to implement SVM using Gradient Descent in Python
class SVM_PREDICTION():

  def calculatin_Hyperparamenter(self,earlier_hyperparamter,x_train,y_classes,learning_rate,hyperParameter,regularization_paramter):
      length_of_early = len(earlier_hyperparamter)
      test_data =0
      for each_iter in range(length_of_early):
        test_data =0

        #with respect to the hyperparamter and the full data set length the test_data is being divided into differnet sets.
        if length_of_early > 0:
          test_data = 1000
        else:
          test_data = 2000

        #calculating the hyperameters with respect to the earlier hypermeters in the arguments and the applying sigmoid function to get the values between 0 and 1
        accumulate_value = 0 
        for train_iter in range(len(x_train)):
          delta = 0.0
          x_intial = x_train[train_iter]
          for del_cal in range(len(earlier_hyperparamter)):
            delta = delta + earlier_hyperparamter[del_cal]*x_intial[del_cal]

          accumulate_value = accumulate_value + (self.sigmoid(delta) - y_classes[train_iter])*x_train[train_iter][each_iter]

        #All the hyperparameters are being being updated.
        hyperParameter[regularization_paramter][each_iter] = hyperParameter[regularization_paramter][each_iter] - learning_rate*accumulate_value


  def sigmoid(self,z_index_value):
    '''
      In this the sigmoid function is being applied and calculated
    '''
    return 1.0/(1.0 + exp(-z_index_value))
  
  
  def calulate_prediction(self,y_pred, y_test):
    '''
      This method will calcualte the confusion matrix with repsective to the original labels and the predicted labels .
      True Positive , False Positive, True Negative and False Negative. Also the classification report .
    '''

    y_pred_list = []
    y_test_list = []
    
    for y_temp_pred, y_temp_test in zip(y_pred,y_test):
      if y_temp_pred[0]==1:
        y_pred_list.append(0)
      else:
        y_pred_list.append(1)
      
      if y_temp_test[0]==1:
        y_test_list.append(0)
      else:
        y_test_list.append(1)

    # Printing Confusion Matrix for SVM
    print(" Confusion Matrix " ,confusion_matrix(y_test_list,y_pred_list))
    tn, fp, fn, tp = confusion_matrix(y_test_list, y_pred_list).ravel()
    
    preci = tp/(tp+fp)
    recal = tp/(tp+fn)
    
    print(classification_report(y_test_list,y_pred_list))

    return preci,recal

  def fit(self,x,y,list_classes,test_train_split,learning_rate,iteration):
    '''
      In this method once the preprocessing is done on the data. It is split into training and test set in these method after which 
      the hyperparameters are calculated .
    '''
    list_of_accuracies = []
    for each_iter in range(1):
      number_of_classes = []
      seed(each_iter+1)
      

      #Sampling out the train and the test samples
      x_test = x.tolist()
      y_test = y.tolist()
    
      
      negate_split = 1 - test_train_split
      #Getting the Train-test split functionality wiht respect to 0.2
      
      x_train = []
      
      length_test = len(x_test)
      y_train = []
      train_size = floor(negate_split*length_test)
      
      while(len(x_train) < train_size):
        index = randrange(len(x_test))
        x_test_value = x_test.pop(index)
        temp_values_test = 0
        y_test_value = y_test.pop(index)
        

        if train_size > 0:
          temp_values_test = 1000
        else:
          temp_values_test = 2000
          
        x_train.append(x_test_value)
        y_train.append(y_test_value)
      
      for iter in range(len(list_classes)):
        temp = []
        for iter_check in y_train:
          temp.append(iter_check[iter])
        
        number_of_classes.append(temp)
        
      hyperParameter=[]
      for ipo in range(len(number_of_classes)):
        value_temp =list(repeat(0, len(x_train[0])))
        hyperParameter.append(value_temp)

      print("Train Samples :",len(x_train))
      print("Test Samples : ",len(x_test))

      #calculating the hyperparamteres with respect to the number of features in the dataset
      for i in range(iteration):
        for each_class_iter in range(len(number_of_classes)):
          earlier_hyperparamter = hyperParameter[each_class_iter]
          self.calculatin_Hyperparamenter(earlier_hyperparamter,x_train,number_of_classes[each_class_iter],learning_rate,hyperParameter,each_class_iter)
      
      #print(len(hyperParameter))
   
      y_pred = self.predict(x_test,hyperParameter)
      y_pred_length = len(y_pred)
      #print(np.unique(y_pred),np.unique(y_test))
      print("Final Statistics")
      print("Linear SVM using Gradient Descent")
      precision ,recall = self.calulate_prediction(y_pred, y_test)
      right_guess = 0 
      for iter in range(y_pred_length):
        prediction_values = y_pred[iter]
        actual_values = y_test[iter]
        if(prediction_values==actual_values):
          right_guess = right_guess + 1

      temp_accuracy = right_guess/y_pred_length
      list_of_accuracies.append(temp_accuracy)
    
    #tn, fp, fn, tp = confusion_matrix(y_test, y_pred)
    print("Validation",each_iter + 1,"accuracy score: ",list_of_accuracies)
    #confusionMatrix = [tn,fp,fn,tp]
    return sum(list_of_accuracies) / len(list_of_accuracies)

  def predict(self,data,hyperParameter):
    '''
      This method will calculate the prediction of the test set with the calculated hyperparamters w and b. 
    '''
    values_of_count = 1
    list_of_prediction = []
    for first_iter in data:

      temporary_parameter = []
      checking_class = [0]*len(hyperParameter)

      for range_iter in range(len(hyperParameter)):

        delta_bias = 0.0
        x_intial = hyperParameter[range_iter]
        length_of_an_iter =len(first_iter)

        for del_cal in range(length_of_an_iter):

          first_iter_value = first_iter[del_cal]

          value_of_w = x_intial[del_cal]

          total_value = first_iter_value * value_of_w
          
          delta_bias = delta_bias + total_value

        temp_sigmod_value=self.sigmoid(delta_bias)

        temporary_parameter.append(temp_sigmod_value)
      
      max_value = max(temporary_parameter)
      training_dataset = 0
      
      position_in_list = temporary_parameter.index(max_value)
      if max_value > 0:
        checking_class[position_in_list] = 0
      else:
        checking_class[position_in_list] = 1

      position_in_list = temporary_parameter.index(max_value)
      checking_class[position_in_list] = 1
      list_of_prediction.append(checking_class)
      values_of_count = values_of_count + 1
    
    return list_of_prediction
    
  def preprocessing(self):
    '''
      Reading from the File movies.csv file and store.
    '''
    print("Running Movie Success using SVM\n")
    url = "movies.csv"
    dataset = pd.read_csv(url)
    return dataset
  


  def getTrainData(self,x):
    '''
    Feature Extraction based upon the correlation with respect to the label is done .
    Categorical values are changed to numerical codes so that it can be used.

    '''
    x = pd.DataFrame(dataset,columns=['budget','genres','original_language','popularity','revenue','vote_average','vote_count'])
    x['genres'] = pd.Categorical(x['genres']).codes
    x['original_language'] = pd.Categorical(x['original_language']).codes
    print(x.describe())
    y = dataset.iloc[:,20:21]
    y['Verdict'] = pd.Categorical(y['Verdict']).codes
    x_data  = x.values
    y = y.values
    print(x_data.shape)
    y=y.reshape(-1)
    return x_data,y

svm_pred = SVM_PREDICTION()
dataset=svm_pred.preprocessing()
dataset.head()

x_data,y=svm_pred.getTrainData(dataset)
x_data = (x_data - x_data.min())/(x_data.max()-x_data.min())

list_classes = list(set(y))
list_classes.sort()
newList = []
for num_iter in range(len(y)): 
  value = [0]
  length_classes = len(list_classes)
  created_one = value*length_classes
  test_dataset = 0
  index_of_created_one = list_classes.index(y[num_iter])
  created_one[index_of_created_one] = 1
  newList.append(created_one)

y_data = np.asarray(newList)
end_accuracy = svm_pred.fit(x_data,y_data,list_classes,test_train_split=0.2,learning_rate=0.02,iteration=300)



